{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before starting..\n",
    "Go to https://cemsbv.crux-nuclei.com/#/ to make an account if you didn't do it yet.\n",
    "\n",
    "To check the available endpoints and their definition go to https://cemsbv.crux-nuclei.com/#/gef-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Cpt interpretation\n",
    "______________________________________________________________________\n",
    "\n",
    "This notebook shows how the cpt model can be used for classifying cpt's. The model is a neural network that can map both the cpt's location as standard cpt features to a soil distribution. The features that it uses are:\n",
    "\n",
    "- Cone resistance: $qc$\n",
    "- Depth:\n",
    "- Friction ratio: $R_f$\n",
    "\n",
    "The model is trained on a certain amount of location clusters. The model has learning location specific soil biases per cluster. It is advised to use this information when making predictions. However you have the freedom to turn it off. This notebook we'll go through getting inference results from the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the next cell to import the libraries that are required, we are using :\n",
    "\n",
    "- `nuclei` https://github.com/cemsbv/nuclei\n",
    "- `pygef` https://github.com/cemsbv/pygef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we import the libraries that are needed\n",
    "from nuclei import call_endpoint, get_endpoints\n",
    "from pygef import Cpt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The soil classification app is called gef-model\n",
    "APP = \"gef-model\"\n",
    "\n",
    "# The endpoints exposed by the model\n",
    "get_endpoints(APP)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gef file\n",
    "In the next cell we parse the gef file with the help of `pygef` and convert it into a `cpt_object`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse the cpt file in GEF object\n",
    "cpt = Cpt(\"cpt_test.gef\")\n",
    "\n",
    "if not cpt.groundwater_level:\n",
    "    cpt.groundwater_level = cpt.zid\n",
    "\n",
    "# create cpt_object for cpt classification\n",
    "cpt_object =  {\n",
    "    \"name\": cpt.test_id,\n",
    "    \"x\": cpt.x,\n",
    "    \"y\": cpt.y,\n",
    "    \"ref\": cpt.zid,\n",
    "    \"index\": np.array(cpt.df[\"elevation_with_respect_to_nap\"], dtype=float),\n",
    "    \"qc\": np.array(cpt.df[\"qc\"], dtype=float).clip(0),\n",
    "    \"fs\": np.array(cpt.df[\"fs\"], dtype=float).clip(0),\n",
    "    \"groundwater_level\": cpt.groundwater_level,\n",
    "}\n",
    "# The api is not yet working properly with the cpt_object so for the time being we will use the cpt_content, \n",
    "# but keep in mind that cpt_object will be the future ;)\n",
    "\n",
    "cpt_content = cpt.s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Schema\n",
    "\n",
    "The [api console](https://cemsbv.crux-nuclei.com/#/gef-model/) for this app describes the schema you'll need for calling the api.\n",
    "The schema defines the following parameters:\n",
    "\n",
    "- cpt_content\n",
    "- include_features (optional default: True)\n",
    "- include_location (optional default: True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"cpt_content\": cpt_content,\n",
    "    \"include_features\": True,\n",
    "    \"include_location\": True,\n",
    "}\n",
    "\n",
    "call_endpoint(APP, \"/plot\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result\n",
    "\n",
    "As you can see the plot seems like a very reasonable classification!\n",
    "\n",
    "The classification colors are:\n",
    "\n",
    "- GREY: Gravel\n",
    "- YELLOW: Sand\n",
    "- BLUE: Silt\n",
    "- GREEN: Clay\n",
    "- BROWN: Peat\n",
    "\n",
    "We can only get numeric results by calling the `/classify` endpoint with the same schema. The result is a dictionary containing (among others) the following keys:\n",
    "\n",
    "- cluster_distances (list): Distance to top N closest clusters in meters.\n",
    "- in_cluster (boolean): Whether the cpt was taken in a known cluster or not.\n",
    "- prediction (pandas DataFrame): Prediction result.\n",
    "\n",
    "By keeping an eye out to `in_cluster` and `cluster_distances` we know if the model is based on data in that cluster or on surrounding clusters. If the `in_cluster` evaluates to True, the model bases its prediction on that cluster only. If not it takes a weighted average of surrounding clusters. The weights are determined by the distance to the cluster centroids. It is recommended to choose 3 clusters as standard.\n",
    "\n",
    "Below you see the boundaries and centroids of the current clusters.\n",
    "\n",
    "![](img/clusters.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = call_endpoint(APP, \"/classify\", schema)\n",
    "print(result.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"prediction\"].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Location bias\n",
    "\n",
    "You can choose to set the features off. In that case the model only predicts based on the location information. This way you'll get an insight in the location bias of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema[\"include_features\"] = False\n",
    "\n",
    "call_endpoint(APP, \"/plot\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No location\n",
    "\n",
    "You can also choose to turn off location information during embedding. As a default, this is not recommended, but if you are affraid that the location bias has too much influence for a certain location, you can turn it off. Below we'll see that result. The prediciton now shows more Gravel, than when the location is included as conditional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"cpt_content\": cpt_content,\n",
    "    \"include_features\": True,\n",
    "    \"include_location\": False,\n",
    "}\n",
    "\n",
    "call_endpoint(APP, \"/plot\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping\n",
    "\n",
    "We can also aggregate layers by setting `aggregate_layers_penalty` > 0. This parameter dictates how many layers are defined. A higher value leads to less layers than lower values. Don't set this value too high as you throw away information with aggregating. If you choose to group your input, a recommended value is between 1 and 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"aggregate_layers_penalty\": 3,\n",
    "    \"cpt_content\": cpt_content,\n",
    "    \"include_features\": True,\n",
    "    \"include_location\": True,\n",
    "}\n",
    "\n",
    "call_endpoint(APP, \"/plot\", schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the NEN classification\n",
    "\n",
    "After the grouping you can also get the **NEN classification** from the classify method, this might be usefull for a fast first classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"aggregate_layers_penalty\": 3,\n",
    "    \"cpt_object\": cpt_object,\n",
    "    \"include_features\": True,\n",
    "    \"include_location\": True,\n",
    "    \"merge_nen_table\": True,\n",
    "    \"interpolate_nen_table_values\": True,\n",
    "}\n",
    "\n",
    "result = call_endpoint(APP, \"/classify\", schema)\n",
    "result[\"layer_table\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check the new result keys:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[\"changepoint_depths\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
